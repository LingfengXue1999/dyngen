---
title: "On runtime and scalability"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{On runtime and scalability}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

<!-- github markdown built using 
rmarkdown::render("vignettes/advanced_constructing_backbone.Rmd", output_format = rmarkdown::github_document())
-->

In this vignette, we will take a look at the runtime of dyngen as the number of genes and the number of cells sampled
is increased. To this end, we'll be using the bifurcating cycle backbone 
which is well known for its beautiful 3D butterfly shape!

An example of a resulting dyngen model is shown here. We tweaked some of the parameters by running this particular backbone once with `num_cells = 100` and `num_features = 100` and verifying that the new parameters still yield the desired outcome. The parameters we tweaked are:

* On average, 10 cells are sampled per simulation (`num_simulations = 100` and `num_cells = 1000`). You could increase this ratio to get a better cell count yield from a given set of simulations, but cells from the same simulation that are temporally close will have highly correlated expression profiles.
* Increased time steps `tau`. This will make the Gillespie algorithm slighty faster but might result in unexpected artifacts in the simulated data. 
* `census_interval` increased from 4 to 10. This will cause dyngen to store an expression profile only every 10 time units. Since the total simulation time is xxx, each simulation will result in yyy data points. Note that on average only 10 data points are sampled per simulation.

## Single run, 10k cells, 10k features
```{r bblego, fig.width = 8, fig.height = 4}
library(dyngen)
library(tidyverse)

set.seed(1)

backbone <- backbone_bifurcating_cycle()

if (!file.exists("scalability_and_runtime_large.rds")) {
  num_cells <- 10000
  num_features <- 10000
  num_tfs <- nrow(backbone$module_info)
  num_targets <- round((num_features - num_tfs) / 2)
  num_hks <- num_features - num_targets - num_tfs
  
  model <- 
    initialise_model(
      backbone = backbone,
      num_tfs = num_tfs,
      num_targets = num_targets,
      num_hks = num_hks,
      num_cells = num_cells,
      gold_standard_params = gold_standard_default(
        census_interval = 1,
        tau = 100/3600
      ),
      simulation_params = simulation_default(
        census_interval = 10,
        ssa_algorithm = ssa_etl(tau = 300/3600),
        experiment_params = simulation_type_wild_type(
          num_simulations = 100
        )
      ),
      verbose = TRUE
    ) %>% 
    generate_tf_network() %>% 
    generate_feature_network() %>% 
    generate_kinetics() %>% 
    generate_gold_standard() %>%  
    generate_cells() %>% 
    generate_experiment()
  
  timings0 <- 
    get_timings(model) %>% 
    mutate(name = forcats::fct_rev(forcats::fct_inorder(paste0(group, " - ", task))))
  
  write_rds(timings0, "scalability_and_runtime_large.rds")
  # write_rds(timings0, "vignettes/scalability_and_runtime_large.rds", compress = "gz")
}

ggplot(timings0) + 
  geom_bar(aes(x = name, y = time_elapsed, fill = group), stat = "identity") +
  scale_fill_brewer(palette = "Dark2") + 
  theme_classic() +
  coord_flip() + labs(x = NULL, y = "Time (s)", fill = "dyngen stage")
```

## Increasing the number of cells

```{r numcells}
settings <- crossing(
  num_cells = seq(1000, 10000, by = 1000),
  num_features = 100,
  rep = seq_len(3)
)

if (!file.exists("scalability_and_runtime_numcells.rds")) {
  timings1 <- pmap_dfr(settings, function(num_cells, num_features, rep) {
    cat("Running num_cells: ", num_cells, ", num_features: ", num_features, ", rep: ", rep, "\n", sep = "")
    num_tfs <- nrow(backbone$module_info)
    num_targets <- round((num_features - num_tfs) / 2)
    num_hks <- num_features - num_targets - num_tfs
    
    out <- 
      initialise_model(
        backbone = backbone,
        num_tfs = num_tfs,
        num_targets = num_targets,
        num_hks = num_hks,
        num_cells = num_cells,
        gold_standard_params = gold_standard_default(
          census_interval = 1,
          tau = 100/3600
        ),
        simulation_params = simulation_default(
          census_interval = 10,
          ssa_algorithm = ssa_etl(tau = 300/3600),
          experiment_params = simulation_type_wild_type(
            num_simulations = num_cells / 10
          )
        ),
        verbose = FALSE
      ) %>% 
      generate_dataset()
    
    tim <- 
      get_timings(out$model) %>% 
      mutate(rep, num_cells, num_features)
    
    cat("  time elapsed: ", sum(tim$time_elapsed), "\n", sep = "")
    
    tim
  })
  write_rds(timings1, "scalability_and_runtime_numcells.rds")
  # write_rds(timings1, "vignettes/scalability_and_runtime_numcells.rds", compress = "gz")
}


```


```{r figure1, fig.width=10, fig.height=5}
timings1 <- read_rds("scalability_and_runtime_numcells.rds")

timings1 %>% 
  group_by(group, task, num_cells, num_features) %>% 
  summarise(time_elapsed = mean(time_elapsed), groups = "drop") %>% 
  group_by(num_cells, num_features, group) %>% summarise(time_elapsed = sum(time_elapsed), .groups = "drop") %>% 
  ggplot() + 
  geom_bar(aes(x = forcats::fct_inorder(as.character(num_cells)), y = time_elapsed, fill = forcats::fct_inorder(group)), stat = "identity") +
  theme_classic() +
  scale_fill_brewer(palette = "Dark2") +
  labs(x = "Number of cells", y = "Average time (s)", fill = "dyngen step")
```


## Increasing the number of features

```{r numfeatures}
settings <- crossing(
  num_cells = 100,
  num_features = seq(1000, 10000, by = 1000),
  rep = seq_len(3)
)

if (!file.exists("scalability_and_runtime_numfeatures.rds")) {
  timings2 <- pmap_dfr(settings, function(num_cells, num_features, rep) {
    cat("Running num_cells: ", num_cells, ", num_features: ", num_features, ", rep: ", rep, "\n", sep = "")
    num_tfs <- nrow(backbone$module_info)
    num_targets <- round((num_features - num_tfs) / 2)
    num_hks <- num_features - num_targets - num_tfs
    
    out <- 
      initialise_model(
        backbone = backbone,
        num_tfs = num_tfs,
        num_targets = num_targets,
        num_hks = num_hks,
        num_cells = num_cells,
        gold_standard_params = gold_standard_default(
          census_interval = 1,
          tau = 100/3600
        ),
        simulation_params = simulation_default(
          census_interval = 10,
          ssa_algorithm = ssa_etl(tau = 300/3600),
          experiment_params = simulation_type_wild_type(
            num_simulations = num_cells / 10
          )
        ),
        verbose = FALSE
      ) %>% 
      generate_dataset()
    
    tim <- 
      get_timings(out$model) %>% 
      mutate(rep, num_cells, num_features)
    
    cat("  time elapsed: ", sum(tim$time_elapsed), "\n", sep = "")
    
    tim
  })
  write_rds(timings2, "scalability_and_runtime_numfeatures.rds")
  # write_rds(timings2, "vignettes/scalability_and_runtime_numfeatures.rds", compress = "gz")
}
```

```{r figure2, fig.width=10, fig.height=5}
timings2 <- read_rds("scalability_and_runtime_numfeatures.rds")

timings2 %>% 
  group_by(group, task, num_cells, num_features) %>% 
  summarise(time_elapsed = mean(time_elapsed), groups = "drop") %>% 
  group_by(num_cells, num_features, group) %>% summarise(time_elapsed = sum(time_elapsed), .groups = "drop") %>% 
  ggplot() + 
  geom_bar(aes(x = forcats::fct_inorder(as.character(num_features)), y = time_elapsed, fill = forcats::fct_inorder(group)), stat = "identity") +
  theme_classic() +
  scale_fill_brewer(palette = "Dark2") +
  labs(x = "Number of features", y = "Average time (s)", fill = "dyngen step")
```


## Ratio between `num_cells` and `num_simulations` is too large`


## `tau` is too large